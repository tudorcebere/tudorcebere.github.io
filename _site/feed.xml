<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tufte-Jekyll</title>
    <description>A Jekyll theme for content-rich sites</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 04 Mar 2025 05:43:48 -0500</pubDate>
    <lastBuildDate>Tue, 04 Mar 2025 05:43:48 -0500</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>Resources I found useful as 1st year PhD student</title>
        <description>&lt;!--more--&gt;
&lt;p&gt;Below are some resources that helped me build a good foundation in probability, measure theory, and related topics in applied mathematics.&lt;/p&gt;

&lt;p&gt;First, I revisited the basics of probability through &lt;a href=&quot;https://www.youtube.com/playlist?list=PLo4jXE-LdDTQq8ZyA8F8reSQHej3F6RFX&quot;&gt;Stanford CS109&lt;/a&gt;. This course is a clear and concise introduction, which helped me recall essential concepts.&lt;/p&gt;

&lt;p&gt;To dive deeper into theory, I watched the &lt;a href=&quot;https://www.youtube.com/playlist?list=PLBh2i93oe2quIJS-j1NpbzEvQCmN00F5o&quot;&gt;Bright Side of Mathematics Measure Theory course&lt;/a&gt;. It does a great job explaining &lt;em&gt;why&lt;/em&gt; measure theory is important, which motivated me to keep learning. I also recommend &lt;a href=&quot;https://www.youtube.com/watch?v=qGsHiHwgInU&amp;amp;list=PLV3oHJg9b1NRhjCs7ZgkAj6US-8m2ymTB&quot;&gt;Professor Lanchier’s Advanced Probability&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/playlist?list=PLo4jXE-LdDTQq8ZyA8F8reSQHej3F6RFX&quot;&gt;Claudio Landim’s Measure Theory course&lt;/a&gt; for a more detailed approach.&lt;/p&gt;

&lt;p&gt;After covering the basics, I looked at applications of probability in different fields. &lt;a href=&quot;https://www.youtube.com/watch?v=9otUB3WXB8E&amp;amp;list=PLLyj1Zd4UWrP3rME2XvFvE4Q5vI3H_7_Z&quot;&gt;Markov Processes by J.N. Corcoran&lt;/a&gt; is an easy-to-follow introduction to many core ideas in applied probability, and Corcoran’s &lt;a href=&quot;https://www.youtube.com/watch?v=ELgjmaSGsWs&amp;amp;list=PLLyj1Zd4UWrOk5-wIki_oOxHJnNj0_437&quot;&gt;Mathematical Statistics&lt;/a&gt; lays out key topics from statistics. &lt;a href=&quot;https://www.math.uci.edu/~rvershyn/teaching/hdp/hdp.html&quot;&gt;R. Vershynin’s High Dimensional Probability&lt;/a&gt; is a bit more advanced but offers a introduction to key tools like concentration inequalities and highdimensional tools in probability. If you want a solid introduction to theoretical machine learning, &lt;a href=&quot;https://www.youtube.com/watch?v=b5NlRg8SjZg&amp;amp;list=PLPW2keNyw-usgvmR7FTQ3ZRjfLs5jT4BO&quot;&gt;S.B. David’s Learning Theory&lt;/a&gt; is excellent.&lt;/p&gt;

&lt;p&gt;Since ideas from information theory show up in both machine learning and differential privacy, I decided to study &lt;a href=&quot;https://www.youtube.com/watch?v=BCiZc0n6COY&amp;amp;list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6&quot;&gt;D. MacKay’s Information Theory, pattern recognition and neural networks&lt;/a&gt; along with the book from the same author.&lt;/p&gt;

&lt;p&gt;When it comes to understanding differential privacy, &lt;a href=&quot;https://www.youtube.com/watch?v=FJMjNOcIqkc&amp;amp;list=PLmd_zeMNzSvRRNpoEWkVo6QY_6rR3SHjp&quot;&gt;G. Kamath’s Differential Privacy&lt;/a&gt; is a clear introduction to this field. Another course that I found fascinating was &lt;a href=&quot;https://sketchingbigdata.org/fall20/lec/&quot;&gt;J. Nelson’s Sketching Algorithms&lt;/a&gt;, which covers a wide range of advanced topics in probabilistic algorithms. If you need optimization methods for machine learning (which can also apply to differential privacy!), check out &lt;a href=&quot;https://www.youtube.com/playlist?list=PL4O4bXkI-fAeYrsBqTUYn2xMjJAqlFQzX&quot;&gt;M. Jaggi’s Optimization for Machine Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Strong writing skills are important for sharing ideas. I found &lt;a href=&quot;https://www.bartleby.com/lit-hub/the-elements-of-style&quot;&gt;The Elements of Style&lt;/a&gt; and &lt;a href=&quot;https://www.microsoft.com/en-us/research/academic-program/write-great-resepuarch-paper/&quot;&gt;How to write a great paper by S.P. Jones&lt;/a&gt; helpful for improving my writing style. I also enjoyed reading Oded Goldreich’s blog, such as &lt;a href=&quot;https://www.wisdom.weizmann.ac.il/~oded/writing.html&quot;&gt;How to write a paper&lt;/a&gt; and &lt;a href=&quot;https://www.wisdom.weizmann.ac.il/~oded/on-ideas.html&quot;&gt;On the emotional difficulty of conducting research&lt;/a&gt;. These pieces made me more aware of both the technical and personal sides of research.&lt;/p&gt;
</description>
        <pubDate>Sun, 29 Oct 2023 00:00:00 -0400</pubDate>
        <link>/articles/23/1st_year_resources</link>
        <guid isPermaLink="true">/articles/23/1st_year_resources</guid>
        
        <category>learning resources</category>
        
        
      </item>
    
      <item>
        <title>Differential Privacy as Hypothesis Testing</title>
        <description>&lt;p&gt;Connecting the dots between Differential Privacy and attackers. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;Differential Privacy (DP) &lt;label for=&quot;1&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;1&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Dwork, C., McSherry, F., Nissim, K., Smith, A. (2006). “Calibrating Noise to Sensitivity in Private Data Analysis”. In: Theory of Cryptography. TCC 2006 &lt;/span&gt; is the golden standard used both in academia and industry to reason about how private an algorithm is. In this post, I’ll give intuition on DP through an attacker’s lens, illustrating how adversaries are left with a measurable chance of leaking privacy.&lt;/p&gt;

&lt;h2 id=&quot;pure-differential-privacy&quot;&gt;Pure Differential Privacy:&lt;/h2&gt;

&lt;p&gt;Differential privacy claims that an algorithm M provides \(\epsilon\)-DP privacy if for two  databases that differ in one element \(D_1\) and \(D_2\) and an output space \(O\), the following property holds:&lt;/p&gt;

\[P[M(D_1) \in O] &amp;lt; e^\epsilon * P[M(D_2) \in O]\]

&lt;p&gt;Dwork and Roth capture the essence of this definition in their seminal textbook:&lt;/p&gt;

&lt;div class=&quot;epigraph&quot;&gt;&lt;blockquote&gt;&lt;p&gt;Differential privacy describes a promise, made by a data holder, or curator, to a data subject: “You will not be affected, adversely or otherwise, by allowing your data to be used in any study or analysis, no matter what other studies, data sets, or information sources, are available.&quot;&lt;/p&gt;&lt;footer&gt;Cynthia Dwork, Aaron Roth, &lt;cite&gt;The Algorithmic Foundations
of DP&lt;/cite&gt;&lt;/footer&gt;&lt;/blockquote&gt;&lt;/div&gt;

&lt;p&gt;&lt;label for=&quot;mf-id-whatever&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;mf-id-whatever&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/blog_resources/dp_ht/spiderman_meme.jpg&quot; /&gt;&lt;br /&gt;If you did not smile, then I recommend reading an introductory material on on Differential Privacy.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In simple terms, changing one individual’s data in the dataset does not significantly alter the distribution of \(M\)’s outputs. Because of this property, an adversary who tries to pinpoint whether your data is included (or what your individual contribution might be) cannot succeed with high confidence. Next, let’s explore what adversaries actually aim to do and how Differential Privacy constrains their efforts.&lt;/p&gt;

&lt;h2 id=&quot;adversaries&quot;&gt;Adversaries&lt;/h2&gt;

&lt;p&gt;Differential privacy is the standard for data privacy because it makes no assumptions. If an algorithm \(M\) is differentially private, an attacker can have &lt;strong&gt;unlimited auxiliary information&lt;/strong&gt; and &lt;strong&gt;unbounded computational power&lt;/strong&gt;, yet they still cannot reliably determine whether a particular individual’s data was included in the dataset. This is due to statistical indistinguishability: the outputs of \(M\) barely change when one person’s data is swapped in or out. Moreover, DP must hold against all adversaries  \(A\) and for every pair of neighboring datasets \((D_1,D)2)\). The diagram below illustrates the adversary’s objective: to distinguish whether an output came from \(D_1\) or \(D_2\).&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/blog_resources/dp_ht/adversarial_game_of_privacy.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Formally, we can view this as a hypothesis test:&lt;/p&gt;

\[H_0: O \sim M(D_1) \text{  vs  } H_1: O \sim M(D_2)\]

&lt;p&gt;An “error” occurs when the adversary incorrectly guesses which dataset produced \(O\). To assess this, we look at the probability of error in a binary classification framework, where the false-positive rate (FPR) and false-negative rate (FNR) determine how successful the adversary is at telling the two datasets apart.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;True Positive (TP)&lt;/td&gt;
      &lt;td&gt;False Positive (FP)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;True Negative (TN)&lt;/td&gt;
      &lt;td&gt;TPR = \(\frac{TP}{TP + FN}\)&lt;/td&gt;
      &lt;td&gt;FNR = \(\frac{FN}{TP + FN}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;False Negative(FN)&lt;/td&gt;
      &lt;td&gt;FPR = \(\frac{FP}{TN + FP}\)&lt;/td&gt;
      &lt;td&gt;TNR = \(\frac{TN}{TN + FP}\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The seminal work of Wasserman&lt;label for=&quot;1&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;1&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Wasserman, Larry, and Shuheng Zhou. “A statistical framework for differential privacy.” Journal of the American Statistical Association 105.489 (2010): 375-389. &lt;/span&gt; and  Kairouz&lt;label for=&quot;2&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;2&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Kairouz, Peter, Sewoong Oh, and Pramod Viswanath. “The composition theorem for differential privacy.” International conference on machine learning. PMLR, 2015. &lt;/span&gt; shows how \(\epsilon\)-DP bounds these error rates. Specifically, for an \(\epsilon\)-DP algorithm:&lt;/p&gt;

\[FNR + e^\epsilon * FPR &amp;gt; 1\]

\[FPR + e^\epsilon * FNR &amp;gt; 1\]

&lt;p&gt;These inequalities imply that an adversary cannot simultaneously keep both FPR and FNR low. In other words, \(\epsilon\) effectively limits how well an attacker can do, creating “trust regions”, which describe the performance of all adversaries. Plotting these regions for multiple values of epsilon, we get:&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/blog_resources/dp_ht/fnr_fpr_epsilon.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;We observe how, as we increase \(\epsilon\), adversaries with lower (FNR, FPR) pairs are allowed for the mechanism, suggesting the existence of stronger adversaries, giving an interpretable alternative definition for \(\epsilon\). Next, we will give a similar interpretation to another well-known variant of differential privacy, Approximate Differential Privacy.&lt;/p&gt;

&lt;h2 id=&quot;approximate-dp&quot;&gt;Approximate DP&lt;/h2&gt;

&lt;p&gt;** Approximate DP ** is a relaxation of pure DP that introduces a parameter \(\delta\) to account for rare “catastrophic failure” events. This type of relaxation is common in cryptography to achieve practicality, as \(\epsilon\)-DP is both restrictive and hard on utility.  Keeping the notation and above \((\epsilon, \delta)\)-DP is defined as:&lt;/p&gt;

&lt;p&gt;&lt;label for=&quot;mf-id-whatever&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;mf-id-whatever&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/blog_resources/dp_ht/download_(2).png&quot; /&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

\[P[M(D_1) \in O] &amp;lt; e^\epsilon * P[M(D_2) \in O] + \delta\]

&lt;p&gt;Introducing \(\delta\) complicates the overall privacy guarantees, because it adds a second dimension to the analysis. Let’s reinterpret these guarantees through the lens of privacy profiles, which allows us to see how an attacker’s performance “trust region” grows once \(\delta\) is considered. Reusing the notation from before, an equivalent definition in terms of error rates is given by:&lt;/p&gt;

\[FNR + e^\epsilon * FPR &amp;gt; 1 - \delta\]

\[FPR + e^\epsilon * FNR &amp;gt; 1 - \delta\]

&lt;p&gt;Similarly, plotting the above pair of inequalities for various pairs of \((\epsilon, \delta)\):&lt;/p&gt;

&lt;p&gt;&lt;label for=&quot;mf-id-whatever&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;mf-id-whatever&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/blog_resources/dp_ht/jack_nicholson.jpg&quot; /&gt;&lt;br /&gt;If you did not smile, then I recommend reading an introductory material on on Differential Privacy.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Observe how the gap between the blue dotted line and the \(\epsilon=0\) axis changes. When \(\delta = 0\), these lines coincide, matching the pure \(\epsilon\)-DP scenario. As \(\delta\) grows, a new “perfect privacy” zone emerges between the dotted line and the \(\epsilon = 0\) axis. In this zone, we essentially permit a tiny probability of unlikely adversarial behavior, hence the relaxation to Approximate DP. However, \(\delta\) must remain small so that these rare events don’t actually happen.&lt;/p&gt;

&lt;h2 id=&quot;end-note&quot;&gt;End Note&lt;/h2&gt;

&lt;p&gt;I think this is enough information for one sitting, we have a good basis for understanding how adversaries are bounded to performa against \(\epsilon\)-DP or \((\epsilon, \delta)\)-DP mechanisms.  The defender doesn’t care about either the data or the attacker’s knowledge, our defence mechanism is a worst-case scenario one, good luck on breaking that!&lt;/p&gt;

</description>
        <pubDate>Thu, 01 Dec 2022 00:00:00 -0500</pubDate>
        <link>/articles/22/dp-ht</link>
        <guid isPermaLink="true">/articles/22/dp-ht</guid>
        
        <category>Differential Privacy</category>
        
        <category>Hypothesis Testing</category>
        
        
      </item>
    
  </channel>
</rss>
